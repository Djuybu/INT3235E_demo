from bs4 import BeautifulSoup
import requests
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

#define lists of standardized keys
ORIGIN = ['nguy√™n qu√°n', 'qu√™ qu√°n', 'n∆°i sinh']
MEMBERS = ['th√†nh vi√™n', 'th√†nh vi√™n hi·ªán t·∫°i', 'th√†nh vi√™n ch√≠nh th·ª©c']
PAST_MEMBERS = ['c·ª±u th√†nh vi√™n', 'th√†nh vi√™n tr∆∞·ªõc ƒë√¢y']
GENRES = ['th·ªÉ lo·∫°i', 'th·ªÉ lo·∫°i √¢m nh·∫°c', 'phong c√°ch √¢m nh·∫°c']
PUBLISHERS = ['h√£ng ƒëƒ©a']
AWARDS = ['gi·∫£i th∆∞·ªüng']

def remove_characters(text):
    if isinstance(text, str):
        # B·ªè c·ª•m [s·ª≠a|s·ª≠a m√£ ngu·ªìn]
        cleaned = text.replace("[s·ª≠a|s·ª≠a m√£ ngu·ªìn]", "")
        # Xo√° ngo·∫∑c v√† d·∫•u c√°ch/d·∫•u ngo·∫∑c k√©p ·ªü ƒë·∫ßu & cu·ªëi
        cleaned = re.sub(r'^[\s\(\)\[\]\'"]+|[\s\(\)\[\]\'"]+$', '', cleaned)
        return cleaned.strip()
    return text

def get_years_from_string(string):
    # Extract years from a string using regex
    return re.findall(r'\b(19|20)\d{2}\b', string)

def get_years(active_years):
    is_active = False
    if not active_years:
        return None, None

    if isinstance(active_years, str):
        active_years = [active_years]

    start_years = []
    end_years = []

    for period in active_years:
        if not period:
            continue
        p = period.strip()

        # Chu·∫©n ho√° c√°c lo·∫°i dash th√†nh hyphen th∆∞·ªùng
        p = re.sub(r'[‚Äì‚Äî‚àí]', '-', p)

        # üîπ L·∫•y t·∫•t c·∫£ nƒÉm v√† c·∫£ t·ª´ "nay"
        tokens = re.findall(r'\b(?:19|20)\d{2}\b|\b(?:nay|hi·ªán t·∫°i|present|now)\b', p, re.IGNORECASE)

        # N·∫øu kh√¥ng c√≥ token n√†o, th·ª≠ ki·ªÉm tra d·∫°ng ƒë·∫∑c bi·ªát "2015-"
        if not tokens:
            if re.search(r'\b(?:19|20)\d{2}\b\s*-\s*$', p):
                start = int(re.search(r'(?:19|20)\d{2}', p).group())
                start_years.append(start)
            continue

        # X·ª≠ l√Ω token ƒë·∫ßu ti√™n (nƒÉm b·∫Øt ƒë·∫ßu)
        first = tokens[0]
        if re.match(r'(?:19|20)\d{2}', first):
            start_years.append(int(first))

        # X·ª≠ l√Ω token cu·ªëi c√πng (nƒÉm tan r√£)
        last = tokens[-1]
        if re.match(r'(?:19|20)\d{2}', last):
            end_years.append(int(last))
        elif re.match(r'(nay|hi·ªán t·∫°i|present|now)', last, re.IGNORECASE):
            is_active = True
            # n·∫øu l√† 'nay' th√¨ kh√¥ng c√≥ nƒÉm tan r√£
            pass
        elif len(tokens) == 1:
            # ch·ªâ c√≥ m·ªôt nƒÉm, coi l√† ho·∫°t ƒë·ªông trong nƒÉm ƒë√≥
            end_years.append(int(first))

    if not start_years:
        return None, None

    start = min(start_years)
    end = None if is_active else max(end_years)
    return start, end

def extract_tabletype_details(content):
    details = {}

    if content is None:
        return details

    # Duy·ªát qua t·ª´ng <li> trong ul
    for li in content.find_all("li"):
        text = li.get_text(strip=True)

        # Ki·ªÉm tra xem c√≥ d·∫•u ':' hay kh√¥ng
        if ":" in text:
            key, value = text.split(":", 1)  # t√°ch 1 l·∫ßn ƒë·∫ßu ti√™n
            details[key.strip()] = value.strip()

    # print("Extracted details:", details)

    return details

def extract_singles(tbody):
    """
    Tr√≠ch xu·∫•t th√¥ng tin c√°c ƒëƒ©a ƒë∆°n t·ª´ tbody c·ªßa b·∫£ng
    """
    # print("Extracting...")
    songs = []
    rows = tbody.find_all('tr')[1:]  # B·ªè qua h√†ng ti√™u ƒë·ªÅ
    
    current_year = None
    current_album = None
    current_year_span = 0
    current_album_span = 0
    
    for row in rows:
        count = 0
        tds_row = None
        if current_year_span == 0 or current_year == None:
            count += 1
        if current_album_span == 0 or current_album == None:
            count += 1
        if count == 2:
            tds_row = row.find_all('td')
        if count == 1:
            tds_row = row.find('td')
        # Ki·ªÉm tra xem c√≥ c·ªôt nƒÉm kh√¥ng
        if current_year_span == 0 or current_year == None:
            year_cell = tds_row[0] if count == 2 else tds_row
            if year_cell:
                year_text = year_cell.get_text(strip=True)
                if year_text.isdigit():
                    current_year = int(year_text)
                    if year_cell.has_attr('rowspan'):
                        current_year_span = int(year_cell['rowspan']) - 1
                    else:
                        current_year_span = 0
                else:
                    current_year = None
            else:
                current_year = None
        
        else:
            current_year_span -= 1

        if current_album_span == 0 or current_album == None:
            album_cell = tds_row[1] if count == 2 else tds_row
            if album_cell:
                album_text = album_cell.get_text(strip=True)
                current_album = album_text if album_text else None
                if album_cell.has_attr('rowspan'):
                    current_album_span = int(album_cell['rowspan']) - 1
                else:
                    current_album_span = 0
            else:
                current_album = None
        else:
            current_album_span -= 1
        
        # T√¨m c·ªôt title (lu√¥n l√† <th scope="row">)
        title_cell = row.find('th', scope='row')
        if title_cell:
            title = title_cell.get_text(strip=True)
            
            # Ki·ªÉm tra xem c√≥ c·ªôt album kh√¥ng
           
            
            songs.append({
                'type': 'ƒêƒ©a ƒë∆°n',
                'nƒÉm': current_year,
                'ti√™u ƒë·ªÅ': title,
                'album': current_album
            })

    # print(songs)
    
    return songs

def crawl_band_albums(soup):
    albums = []

    for title_tag in soup.select("div.mw-heading.mw-heading2"):
        text = remove_characters(title_tag.get_text(strip=True)).lower()
        print(f"Found heading2: {text}")
        if "album" in text or "ƒëƒ©a nh·∫°c" in text:
            album_type = "Album"
            # Duy·ªát c√°c th·∫ª anh em sau ti√™u ƒë·ªÅ ch√≠nh
            for sib in title_tag.find_next_siblings():
                # N·∫øu g·∫∑p ti√™u ƒë·ªÅ c·∫•p 2 kh√°c => d·ª´ng
                if sib.name == "div" and "mw-heading2" in sib.get("class", []):
                    # print("Encountered another level 2 heading, stopping.")
                    break

                # N·∫øu l√† ti√™u ƒë·ªÅ c·∫•p 3 => lo·∫°i album (studio, live, v.v.)
                if sib.name == "div" and "mw-heading3" in sib.get("class", []):
                    album_type = remove_characters(sib.get_text(strip=True))
                    # print(f"Processing album type: {album_type}")


                if 'ƒêƒ©a ƒë∆°n' in sib.get_text(strip=True):
                    # print("Processing 'ƒêƒ©a ƒë∆°n' section")
                    tbody = sib.find('tbody')
                    if tbody:
                        albums.extend(extract_singles(tbody))
                    break


                # N·∫øu g·∫∑p b·∫£ng album
                if sib.name == "table":
                    headers = [th.get_text(strip=True).lower() for th in sib.find_all("th")]
                    if len(headers) == 4 and album_col is not None:
                        album_col = None
                        release_col = None
                        for i, h in enumerate(headers):
                            if "album" in h:
                                album_col = i
                            elif "ph√°t h√†nh" in h:
                                release_col = i
                        for row in rows:
                            cells = row.find_all(["td", "th"])
                            if not cells:
                                continue
                            album["title"] = cells[album_col].get_text(strip=True) if album_col < len(cells) else None
                            album["release_date"] = cells[release_col].get_text(strip=True) if release_col is not None and release_col < len(cells) else None
                    else:
                        rows = sib.find_all("tr")[1:]  # b·ªè h√†ng ti√™u ƒë·ªÅ
                        for row in rows:
                            print("Processing album row")
                            album = {}
                            album["type"] = album_type
                            th = row.find("th")
                            td = row.find("td")

                            # N·ªôi dung chi ti·∫øt trong <ul>
                            ul_tag = td.find("ul") if td else None
                            # print(extract_tabletype_details(ul_tag))
                            album.update(extract_tabletype_details(ul_tag))
                            album['title'] = th.get_text(strip=True) if th else None
                        # print(f"Album title: {album['title']}")
                        for key in album:
                            if album[key]:
                                album[key] = remove_characters(album[key])
                        albums.append(album)
                
                elif sib.name == "ul":
                    print("Processing <ul> list of albums")
                    for li in sib.find_all("li"):
                        text = li.get_text(strip=True)
                        match = re.match(r'^(.*?)\s*\((\d{4})\)$', text)
                        if match:
                            title, year = match.groups()
                            album = {"type": album_type,"title": title.strip(), "year": int(year)}
                            for key in album:
                                if album[key]:
                                    print(album[key])
                                    album[key] = remove_characters(album[key])
                            albums.append(album)
                                
    return albums

# H√†m crawl c√°c trang Web trong ph·∫ßn danh m·ª•c
def crawl_category_website(url):
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Check if the request was successful
        soup = BeautifulSoup(response.text, 'html.parser')

        # Tr·ªè v√†o ph·∫ßn ch·ª©a c√°c url
        singer_links = soup.find("div", {"id": "mw-pages"})

        # L·∫•y t·∫•t c·∫£ c√°c th·∫ª <a> trong ph·∫ßn n√†y
        links = singer_links.find_all('a', href=True)
        urls = ["https://vi.wikipedia.org" + link['href'] for link in links if link['href'].startswith('/wiki/')]
        return urls

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return []
    
def crawl_singer_info(urls):
    singers = []
    for url in urls:
        try:
            print(f"Crawling {url}...")
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            #Tr·ªè v√†o info box
            info_box = soup.find("table", {"class": "infobox"})
            if not info_box:
                continue
            info_rows = info_box.find_all("tr")
            singer_info = {}
            singer_info['name'] = soup.find("h1", {"id": "firstHeading"}).get_text(strip=True)
            for row in info_rows:
                header = row.find("th")
                data = row.find("td")
                if header and data:
                    key = header.get_text(strip=True)

                    # --- TR∆Ø·ªúNG H·ª¢P 1: C√≥ <div class="hlist"> ---
                    hlist_div = data.find("div", {"class": "hlist"})
                    if hlist_div:
                        # L·∫•y n·ªôi dung trong c√°c <li>
                        items = [li.get_text(strip=True) for li in hlist_div.find_all("li")]
                        singer_info[key] = items
                        continue  # b·ªè qua c√°c x·ª≠ l√Ω ti·∫øp theo, v√¨ ƒë√£ c√≥ k·∫øt qu·∫£

                    ul_tag = data.find("ul")
                    if ul_tag:
                        items = [li.get_text(strip=True) for li in ul_tag.find_all("li")]
                        singer_info[key] = items
                        continue

                    # --- TR∆Ø·ªúNG H·ª¢P 2: C√≥ <br> ---
                    if data.find("br"):
                        # split d·ª±a theo th·∫ª <br>
                        parts = [text.strip() for text in data.stripped_strings]
                        singer_info[key] = [p for p in parts if p]  # lo·∫°i b·ªè chu·ªói r·ªóng
                    else:
                        # Tr∆∞·ªùng h·ª£p b√¨nh th∆∞·ªùng
                        value = data.get_text(separator=' ', strip=True)
                        singer_info[key] = value
                    
                    singer_info['nƒÉm th√†nh l·∫≠p'], singer_info['nƒÉm tan r√£'] = get_years(singer_info.get('NƒÉm ho·∫°t ƒë·ªông'))
            # print(f"Singer Info from {url}: {singer_info}")
            singer_info['albums'] = crawl_band_albums(soup)
            singers.append(singer_info)
        except requests.exceptions.RequestException as e:
            print(f"Error fetching {url}: {e}")
    return singers

def beautify_data(singers):
    # print(type(singers))
    for singer in singers:
        # print(singer)
        if 'Nguy√™n qu√°n' in singer and "S√†i G√≤n" in singer.get('Nguy√™n qu√°n'):
            singer['Nguy√™n qu√°n'] = 'TP.H·ªì Ch√≠ Minh, Vi·ªát Nam'
        if not singer.get('H√£ng ƒëƒ©a'):
            singer['H√£ng ƒëƒ©a'] = 'ƒê·ªôc l·∫≠p'
        if isinstance(singer.get('Th√†nh vi√™n'), list):
            for member in singer['Th√†nh vi√™n']:
                if member in ['Xem l·ªãch s·ª≠ th√†nh vi√™n', 'Xem Th√†nh vi√™n', 'Xem l·ªãch s·ª≠ nh√¢n s·ª±']:
                    singer['Th√†nh vi√™n'].remove(member)
        elif isinstance(singer.get('Th√†nh vi√™n'), str):
            if singer['Th√†nh vi√™n'] in ['Xem l·ªãch s·ª≠ th√†nh vi√™n', 'Xem Th√†nh vi√™n', 'Xem l·ªãch s·ª≠ nh√¢n s·ª±']:
                singer['Th√†nh vi√™n'] = None
        if "ban nh·∫°c" in singer.get('name'):
            singer['name'] = re.sub(r"\(ban nh·∫°c\)", "", singer['name'], flags=re.IGNORECASE).strip()
    return singers

def export_to_json(singers, filename='singers.json'):
    import json
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(singers, f, ensure_ascii=False, indent=4)

urls = crawl_category_website("https://vi.wikipedia.org/wiki/Th%E1%BB%83_lo%E1%BA%A1i:Ban_nh%E1%BA%A1c_Vi%E1%BB%87t_Nam")
bands = crawl_singer_info(urls)
# print(bands)
bands = beautify_data(bands)
export_to_json(bands, 'raw_data/bands.json')
# print(crawl_band_albums(BeautifulSoup(requests.get("https://vi.wikipedia.org/wiki/C%C3%A1_H%E1%BB%93i_Hoang", headers=headers).text, 'lxml')))