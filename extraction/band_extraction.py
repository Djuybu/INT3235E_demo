from bs4 import BeautifulSoup
import requests
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

#define lists of standardized keys
ORIGIN = ['nguyÃªn quÃ¡n', 'quÃª quÃ¡n', 'nÆ¡i sinh']
MEMBERS = ['thÃ nh viÃªn', 'thÃ nh viÃªn hiá»‡n táº¡i', 'thÃ nh viÃªn chÃ­nh thá»©c']
PAST_MEMBERS = ['cá»±u thÃ nh viÃªn', 'thÃ nh viÃªn trÆ°á»›c Ä‘Ã¢y']
GENRES = ['thá»ƒ loáº¡i', 'thá»ƒ loáº¡i Ã¢m nháº¡c', 'phong cÃ¡ch Ã¢m nháº¡c']
PUBLISHERS = ['hÃ£ng Ä‘Ä©a']
AWARDS = ['giáº£i thÆ°á»Ÿng']

def remove_characters(text):
    if isinstance(text, str):
        # Bá» cá»¥m [sá»­a|sá»­a mÃ£ nguá»“n]
        cleaned = text.replace("[sá»­a|sá»­a mÃ£ nguá»“n]", "")
        # XoÃ¡ ngoáº·c vÃ  dáº¥u cÃ¡ch/dáº¥u ngoáº·c kÃ©p á»Ÿ Ä‘áº§u & cuá»‘i
        cleaned = re.sub(r'^[\s\(\)\[\]\'"]+|[\s\(\)\[\]\'"]+$', '', cleaned)
        return cleaned.strip()
    return text

def get_years_from_string(string):
    # Extract years from a string using regex
    return re.findall(r'\b(19|20)\d{2}\b', string)

def get_years(active_years):
    is_active = False
    if not active_years:
        return None, None

    if isinstance(active_years, str):
        active_years = [active_years]

    start_years = []
    end_years = []

    for period in active_years:
        if not period:
            continue
        p = period.strip()

        # Chuáº©n hoÃ¡ cÃ¡c loáº¡i dash thÃ nh hyphen thÆ°á»ng
        p = re.sub(r'[â€“â€”âˆ’]', '-', p)

        # ğŸ”¹ Láº¥y táº¥t cáº£ nÄƒm vÃ  cáº£ tá»« "nay"
        tokens = re.findall(r'\b(?:19|20)\d{2}\b|\b(?:nay|hiá»‡n táº¡i|present|now)\b', p, re.IGNORECASE)

        # Náº¿u khÃ´ng cÃ³ token nÃ o, thá»­ kiá»ƒm tra dáº¡ng Ä‘áº·c biá»‡t "2015-"
        if not tokens:
            if re.search(r'\b(?:19|20)\d{2}\b\s*-\s*$', p):
                start = int(re.search(r'(?:19|20)\d{2}', p).group())
                start_years.append(start)
            continue

        # Xá»­ lÃ½ token Ä‘áº§u tiÃªn (nÄƒm báº¯t Ä‘áº§u)
        first = tokens[0]
        if re.match(r'(?:19|20)\d{2}', first):
            start_years.append(int(first))

        # Xá»­ lÃ½ token cuá»‘i cÃ¹ng (nÄƒm tan rÃ£)
        last = tokens[-1]
        if re.match(r'(?:19|20)\d{2}', last):
            end_years.append(int(last))
        elif re.match(r'(nay|hiá»‡n táº¡i|present|now)', last, re.IGNORECASE):
            is_active = True
            # náº¿u lÃ  'nay' thÃ¬ khÃ´ng cÃ³ nÄƒm tan rÃ£
            pass
        elif len(tokens) == 1:
            # chá»‰ cÃ³ má»™t nÄƒm, coi lÃ  hoáº¡t Ä‘á»™ng trong nÄƒm Ä‘Ã³
            end_years.append(int(first))

    if not start_years:
        return None, None

    start = min(start_years)
    end = None if is_active else max(end_years)
    return start, end

def extract_tabletype_details(content):
    details = {}

    if content is None:
        return details

    # Duyá»‡t qua tá»«ng <li> trong ul
    for li in content.find_all("li"):
        text = li.get_text(strip=True)

        # Kiá»ƒm tra xem cÃ³ dáº¥u ':' hay khÃ´ng
        if ":" in text:
            key, value = text.split(":", 1)  # tÃ¡ch 1 láº§n Ä‘áº§u tiÃªn
            details[key.strip()] = value.strip()

    # print("Extracted details:", details)

    return details

def extract_singles(tbody):
    """
    TrÃ­ch xuáº¥t thÃ´ng tin cÃ¡c Ä‘Ä©a Ä‘Æ¡n tá»« tbody cá»§a báº£ng
    """
    # print("Extracting...")
    songs = []
    rows = tbody.find_all('tr')[1:]  # Bá» qua hÃ ng tiÃªu Ä‘á»
    
    current_year = None
    current_album = None
    current_year_span = 0
    current_album_span = 0
    
    for row in rows:
        count = 0
        tds_row = None
        if current_year_span == 0 or current_year == None:
            count += 1
        if current_album_span == 0 or current_album == None:
            count += 1
        if count == 2:
            tds_row = row.find_all('td')
        if count == 1:
            tds_row = row.find('td')
        # Kiá»ƒm tra xem cÃ³ cá»™t nÄƒm khÃ´ng
        if current_year_span == 0 or current_year == None:
            year_cell = tds_row[0] if count == 2 else tds_row
            if year_cell:
                year_text = year_cell.get_text(strip=True)
                if year_text.isdigit():
                    current_year = int(year_text)
                    if year_cell.has_attr('rowspan'):
                        current_year_span = int(year_cell['rowspan']) - 1
                    else:
                        current_year_span = 0
                else:
                    current_year = None
            else:
                current_year = None
        
        else:
            current_year_span -= 1

        if current_album_span == 0 or current_album == None:
            album_cell = tds_row[1] if count == 2 else tds_row
            if album_cell:
                album_text = album_cell.get_text(strip=True)
                current_album = album_text if album_text else None
                if album_cell.has_attr('rowspan'):
                    current_album_span = int(album_cell['rowspan']) - 1
                else:
                    current_album_span = 0
            else:
                current_album = None
        else:
            current_album_span -= 1
        
        # TÃ¬m cá»™t title (luÃ´n lÃ  <th scope="row">)
        title_cell = row.find('th', scope='row')
        if title_cell:
            title = title_cell.get_text(strip=True)
            
            # Kiá»ƒm tra xem cÃ³ cá»™t album khÃ´ng
           
            
            songs.append({
                'type': 'ÄÄ©a Ä‘Æ¡n',
                'nÄƒm': current_year,
                'tiÃªu Ä‘á»': title,
                'album': current_album
            })

    # print(songs)
    
    return songs

def crawl_band_albums(soup):
    albums = []

    for title_tag in soup.select("div.mw-heading.mw-heading2"):
        text = remove_characters(title_tag.get_text(strip=True)).lower()
        print(f"Found heading2: {text}")
        if "album" in text or "Ä‘Ä©a nháº¡c" in text:
            album_type = "Album"
            # Duyá»‡t cÃ¡c tháº» anh em sau tiÃªu Ä‘á» chÃ­nh
            for sib in title_tag.find_next_siblings():
                # Náº¿u gáº·p tiÃªu Ä‘á» cáº¥p 2 khÃ¡c => dá»«ng
                if sib.name == "div" and "mw-heading2" in sib.get("class", []):
                    # print("Encountered another level 2 heading, stopping.")
                    break

                # Náº¿u lÃ  tiÃªu Ä‘á» cáº¥p 3 => loáº¡i album (studio, live, v.v.)
                if sib.name == "div" and "mw-heading3" in sib.get("class", []):
                    album_type = remove_characters(sib.get_text(strip=True))
                    # print(f"Processing album type: {album_type}")


                if 'ÄÄ©a Ä‘Æ¡n' in sib.get_text(strip=True):
                    # print("Processing 'ÄÄ©a Ä‘Æ¡n' section")
                    tbody = sib.find('tbody')
                    if tbody:
                        albums.extend(extract_singles(tbody))
                    break


                # Náº¿u gáº·p báº£ng album
                if sib.name == "table":
                    headers = [th.get_text(strip=True).lower() for th in sib.find_all("th")]
                    if len(headers) == 4 and album_col is not None:
                        album_col = None
                        release_col = None
                        for i, h in enumerate(headers):
                            if "album" in h:
                                album_col = i
                            elif "phÃ¡t hÃ nh" in h:
                                release_col = i
                            album["title"] = cells[album_col].get_text(strip=True) if album_col < len(cells) else None
                            album["release_date"] = cells[release_col].get_text(strip=True) if release_col is not None and release_col < len(cells) else None
                    else:
                        rows = sib.find_all("tr")[1:]  # bá» hÃ ng tiÃªu Ä‘á»
                        for row in rows:
                            print("Processing album row")
                            album = {}
                            album["type"] = album_type
                            th = row.find("th")
                            td = row.find("td")

                            # Ná»™i dung chi tiáº¿t trong <ul>
                            ul_tag = td.find("ul") if td else None
                            # print(extract_tabletype_details(ul_tag))
                            album.update(extract_tabletype_details(ul_tag))
                            album['title'] = th.get_text(strip=True) if th else None
                        # print(f"Album title: {album['title']}")
                        for key in album:
                            if album[key]:
                                album[key] = remove_characters(album[key])
                        albums.append(album)
                
                elif sib.name == "ul":
                    print("Processing <ul> list of albums")
                    for li in sib.find_all("li"):
                        text = li.get_text(strip=True)
                        match = re.match(r'^(.*?)\s*\((\d{4})\)$', text)
                        if match:
                            title, year = match.groups()
                            album = {"type": album_type,"title": title.strip(), "year": int(year)}
                            for key in album:
                                if album[key]:
                                    print(album[key])
                                    album[key] = remove_characters(album[key])
                            albums.append(album)
                                
    return albums

# HÃ m crawl cÃ¡c trang Web trong pháº§n danh má»¥c
def crawl_category_website(url):
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Check if the request was successful
        soup = BeautifulSoup(response.text, 'lxml')

        # Trá» vÃ o pháº§n chá»©a cÃ¡c url
        singer_links = soup.find("div", {"id": "mw-pages"})

        # Láº¥y táº¥t cáº£ cÃ¡c tháº» <a> trong pháº§n nÃ y
        links = singer_links.find_all('a', href=True)
        urls = ["https://vi.wikipedia.org" + link['href'] for link in links if link['href'].startswith('/wiki/')]
        return urls

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return []
    
def crawl_singer_info(urls):
    singers = []
    for url in urls:
        try:
            print(f"Crawling {url}...")
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')

            #Trá» vÃ o info box
            info_box = soup.find("table", {"class": "infobox"})
            if not info_box:
                continue
            info_rows = info_box.find_all("tr")
            singer_info = {}
            singer_info['name'] = soup.find("h1", {"id": "firstHeading"}).get_text(strip=True)
            for row in info_rows:
                header = row.find("th")
                data = row.find("td")
                if header and data:
                    key = header.get_text(strip=True)

                    # --- TRÆ¯á»œNG Há»¢P 1: CÃ³ <div class="hlist"> ---
                    hlist_div = data.find("div", {"class": "hlist"})
                    if hlist_div:
                        # Láº¥y ná»™i dung trong cÃ¡c <li>
                        items = [li.get_text(strip=True) for li in hlist_div.find_all("li")]
                        singer_info[key] = items
                        continue  # bá» qua cÃ¡c xá»­ lÃ½ tiáº¿p theo, vÃ¬ Ä‘Ã£ cÃ³ káº¿t quáº£

                    ul_tag = data.find("ul")
                    if ul_tag:
                        items = [li.get_text(strip=True) for li in ul_tag.find_all("li")]
                        singer_info[key] = items
                        continue

                    # --- TRÆ¯á»œNG Há»¢P 2: CÃ³ <br> ---
                    if data.find("br"):
                        # split dá»±a theo tháº» <br>
                        parts = [text.strip() for text in data.stripped_strings]
                        singer_info[key] = [p for p in parts if p]  # loáº¡i bá» chuá»—i rá»—ng
                    else:
                        # TrÆ°á»ng há»£p bÃ¬nh thÆ°á»ng
                        value = data.get_text(separator=' ', strip=True)
                        singer_info[key] = value
                    
                    singer_info['nÄƒm thÃ nh láº­p'], singer_info['nÄƒm tan rÃ£'] = get_years(singer_info.get('NÄƒm hoáº¡t Ä‘á»™ng'))
            # print(f"Singer Info from {url}: {singer_info}")
            singer_info['albums'] = crawl_band_albums(soup)
            singers.append(singer_info)
        except requests.exceptions.RequestException as e:
            print(f"Error fetching {url}: {e}")
    return singers

def beautify_data(singers):
    # print(type(singers))
    for singer in singers:
        # print(singer)
        if 'NguyÃªn quÃ¡n' in singer and "SÃ i GÃ²n" in singer.get('NguyÃªn quÃ¡n'):
            singer['NguyÃªn quÃ¡n'] = 'TP.Há»“ ChÃ­ Minh, Viá»‡t Nam'
        if not singer.get('HÃ£ng Ä‘Ä©a'):
            singer['HÃ£ng Ä‘Ä©a'] = 'Äá»™c láº­p'
        if isinstance(singer.get('ThÃ nh viÃªn'), list):
            for member in singer['ThÃ nh viÃªn']:
                if member in ['Xem lá»‹ch sá»­ thÃ nh viÃªn', 'Xem ThÃ nh viÃªn', 'Xem lá»‹ch sá»­ nhÃ¢n sá»±']:
                    singer['ThÃ nh viÃªn'].remove(member)
        elif isinstance(singer.get('ThÃ nh viÃªn'), str):
            if singer['ThÃ nh viÃªn'] in ['Xem lá»‹ch sá»­ thÃ nh viÃªn', 'Xem ThÃ nh viÃªn', 'Xem lá»‹ch sá»­ nhÃ¢n sá»±']:
                singer['ThÃ nh viÃªn'] = None
        if "ban nháº¡c" in singer.get('name'):
            singer['name'] = re.sub(r"\(ban nháº¡c\)", "", singer['name'], flags=re.IGNORECASE).strip()
    return singers

def export_to_json(singers, filename='singers.json'):
    import json
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(singers, f, ensure_ascii=False, indent=4)

urls = crawl_category_website("https://vi.wikipedia.org/wiki/Th%E1%BB%83_lo%E1%BA%A1i:Ban_nh%E1%BA%A1c_Vi%E1%BB%87t_Nam")
bands = crawl_singer_info(urls)
# print(bands)
bands = beautify_data(bands)
export_to_json(bands, 'bands.json')
# print(crawl_band_albums(BeautifulSoup(requests.get("https://vi.wikipedia.org/wiki/C%C3%A1_H%E1%BB%93i_Hoang", headers=headers).text, 'lxml')))