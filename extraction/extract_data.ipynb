{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from collections import deque\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcce334",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = r\"../raw_data\"\n",
    "EXCLUDED_PATH = os.path.join(SAVE_DIR, \"excluded_links.json\")\n",
    "DEPTH_LIMIT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75198742",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5744987",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [\"https://vi.wikipedia.org/wiki/L%E1%BB%87_Quy%C3%AAn_(ca_s%C4%A9,_sinh_1981)\", \n",
    "        \"https://vi.wikipedia.org/wiki/Miu_L%C3%AA\", \n",
    "        \"https://vi.wikipedia.org/wiki/H%C3%B2a_Minzy\",\n",
    "        \"https://vi.wikipedia.org/wiki/M%E1%BB%B9_Linh\",\n",
    "        \"https://vi.wikipedia.org/wiki/Only_C\",\n",
    "        \"https://vi.wikipedia.org/wiki/JustaTee\",\n",
    "        \"https://vi.wikipedia.org/wiki/Ch%E1%BA%BF_Linh\",\n",
    "        \"https://vi.wikipedia.org/wiki/%C4%90%C3%A0m_V%C4%A9nh_H%C6%B0ng\",\n",
    "        \"https://vi.wikipedia.org/wiki/Mr._Siro\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters(text):\n",
    "    if isinstance(text, str):\n",
    "        # B·ªè c·ª•m [s·ª≠a|s·ª≠a m√£ ngu·ªìn]\n",
    "        cleaned = text.replace(\"[s·ª≠a|s·ª≠a m√£ ngu·ªìn]\", \"\")\n",
    "        # Xo√° ngo·∫∑c v√† d·∫•u c√°ch/d·∫•u ngo·∫∑c k√©p ·ªü ƒë·∫ßu & cu·ªëi\n",
    "        cleaned = re.sub(r'^[\\s\\(\\)\\[\\]\\'\"]+|[\\s\\(\\)\\[\\]\\'\"]+$', '', cleaned)\n",
    "        return cleaned.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years(active_years):\n",
    "    is_active = False\n",
    "    if not active_years:\n",
    "        return None, None\n",
    "\n",
    "    if isinstance(active_years, str):\n",
    "        active_years = [active_years]\n",
    "\n",
    "    start_years = []\n",
    "    end_years = []\n",
    "\n",
    "    for period in active_years:\n",
    "        if not period:\n",
    "            continue\n",
    "        p = period.strip()\n",
    "\n",
    "        # Chu·∫©n ho√° c√°c lo·∫°i dash th√†nh hyphen th∆∞·ªùng\n",
    "        p = re.sub(r'[‚Äì‚Äî‚àí]', '-', p)\n",
    "\n",
    "        # üîπ L·∫•y t·∫•t c·∫£ nƒÉm v√† c·∫£ t·ª´ \"nay\"\n",
    "        tokens = re.findall(r'\\b(?:19|20)\\d{2}\\b|\\b(?:nay|hi·ªán t·∫°i|present|now)\\b', p, re.IGNORECASE)\n",
    "\n",
    "        # N·∫øu kh√¥ng c√≥ token n√†o, th·ª≠ ki·ªÉm tra d·∫°ng ƒë·∫∑c bi·ªát \"2015-\"\n",
    "        if not tokens:\n",
    "            if re.search(r'\\b(?:19|20)\\d{2}\\b\\s*-\\s*$', p):\n",
    "                start = int(re.search(r'(?:19|20)\\d{2}', p).group())\n",
    "                start_years.append(start)\n",
    "            continue\n",
    "\n",
    "        # X·ª≠ l√Ω token ƒë·∫ßu ti√™n (nƒÉm b·∫Øt ƒë·∫ßu)\n",
    "        first = tokens[0]\n",
    "        if re.match(r'(?:19|20)\\d{2}', first):\n",
    "            start_years.append(int(first))\n",
    "\n",
    "        # X·ª≠ l√Ω token cu·ªëi c√πng (nƒÉm tan r√£)\n",
    "        last = tokens[-1]\n",
    "        if re.match(r'(?:19|20)\\d{2}', last):\n",
    "            end_years.append(int(last))\n",
    "        elif re.match(r'(nay|hi·ªán t·∫°i|present|now)', last, re.IGNORECASE):\n",
    "            is_active = True\n",
    "            # n·∫øu l√† 'nay' th√¨ kh√¥ng c√≥ nƒÉm tan r√£\n",
    "            pass\n",
    "        elif len(tokens) == 1:\n",
    "            # ch·ªâ c√≥ m·ªôt nƒÉm, coi l√† ho·∫°t ƒë·ªông trong nƒÉm ƒë√≥\n",
    "            end_years.append(int(first))\n",
    "\n",
    "    if not start_years:\n",
    "        return None, None\n",
    "\n",
    "    start = min(start_years)\n",
    "    end = None if is_active else max(end_years)\n",
    "    return start, end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excluded_links():\n",
    "    if os.path.exists(EXCLUDED_PATH):\n",
    "        with open(EXCLUDED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            return set(json.load(f))\n",
    "    return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excluded_links(excluded_links):\n",
    "    with open(EXCLUDED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted(list(excluded_links)), f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crawled_links():\n",
    "    \"\"\"ƒê·ªçc danh s√°ch link ƒë√£ t·ª´ng crawl.\"\"\"\n",
    "    if os.path.exists(\"crawled_links.json\"):\n",
    "        with open(\"crawled_links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            return set(json.load(f))\n",
    "    return set()\n",
    "\n",
    "def save_crawled_links(data):\n",
    "    \"\"\"L∆∞u danh s√°ch link ƒë√£ crawl.\"\"\"\n",
    "    with open(\"crawled_links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(data), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# ====== H√ÄM CH√çNH ======\n",
    "def crawl_valid_links(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    stop_headings = [\"Ch√∫ th√≠ch\", \"Tham kh·∫£o\", \"Li√™n k·∫øt ngo√†i\"]\n",
    "\n",
    "    content = soup.find('div', id='mw-content-text')\n",
    "    if not content:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung ch√≠nh trong trang!\")\n",
    "        return []\n",
    "\n",
    "    all_links = []\n",
    "\n",
    "    for element in content.find_all(['p', 'ul', 'ol', 'div', 'h2', 'h3'], recursive=True):\n",
    "        if element.name in ['h2', 'h3']:\n",
    "            heading_text = element.get_text(strip=True)\n",
    "            if any(stop in heading_text for stop in stop_headings):\n",
    "                print(f\"üõë D·ª´ng t·∫°i m·ª•c: {heading_text}\")\n",
    "                break\n",
    "\n",
    "        for link in element.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and not any(x in href for x in [':', '#']):\n",
    "                full_url = \"https://vi.wikipedia.org\" + href\n",
    "                all_links.append(full_url)\n",
    "\n",
    "    # Lo·∫°i b·ªè tr√πng l·∫∑p trong trang hi·ªán t·∫°i\n",
    "    all_links = list(dict.fromkeys(all_links))\n",
    "    print(f\"üîç T√¨m th·∫•y {len(all_links)} ƒë∆∞·ªùng d·∫´n h·ª£p l·ªá trong trang ch√≠nh.\")\n",
    "\n",
    "    excluded_links = load_excluded_links()\n",
    "    crawled_links = load_crawled_links()\n",
    "\n",
    "    print(f\"üìÇ B·ªè qua {len(excluded_links)} link ƒë√£ b·ªã lo·∫°i tr∆∞·ªõc ƒë√≥...\")\n",
    "    print(f\"üß≠ B·ªè qua {len(crawled_links)} link ƒë√£ ƒë∆∞·ª£c crawl tr∆∞·ªõc ƒë√≥...\")\n",
    "\n",
    "    valid_links = []\n",
    "    new_excluded = set()\n",
    "    new_crawled = set()\n",
    "\n",
    "    keywords = [\n",
    "        \"ca sƒ© vi·ªát nam\", \"nam ca sƒ© vi·ªát nam\", \"n·ªØ ca sƒ© vi·ªát nam\",\n",
    "        \"ca sƒ© g·ªëc vi·ªát\", \"ca sƒ© h·∫£i ngo·∫°i\", \"nh·∫°c sƒ© vi·ªát nam\",\n",
    "        \"ban nh·∫°c vi·ªát nam\", \"ban nh·∫°c rock vi·ªát nam\",\n",
    "        \"nh√† s·∫£n xu·∫•t thu √¢m vi·ªát nam\", \"nh√† s·∫£n xu·∫•t √¢m nh·∫°c vi·ªát nam\",\n",
    "        \"nh·∫°c sƒ© h√≤a √¢m ph·ªëi kh√≠ vi·ªát nam\", \"rapper vi·ªát nam\"\n",
    "    ]\n",
    "    keywords_lower = [k.lower() for k in keywords]\n",
    "\n",
    "    for link in all_links:\n",
    "        if link in excluded_links or link in crawled_links:\n",
    "            print(f\"‚è© B·ªè qua (ƒë√£ lo·∫°i ho·∫∑c ƒë√£ crawl): {link}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sub_resp = requests.get(link, headers=headers, timeout=6)\n",
    "            sub_soup = BeautifulSoup(sub_resp.text, 'html.parser')\n",
    "            cat_div = sub_soup.find('div', id='mw-normal-catlinks')\n",
    "\n",
    "            if cat_div:\n",
    "                cat_text = cat_div.get_text(strip=True).lower()\n",
    "                if any(k in cat_text for k in keywords_lower):\n",
    "                    valid_links.append(link)\n",
    "                    print(f\"‚úÖ Gi·ªØ l·∫°i: {link}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Lo·∫°i b·ªè: {link}\")\n",
    "                    new_excluded.add(link)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y danh m·ª•c: {link}\")\n",
    "                new_excluded.add(link)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói khi truy c·∫≠p {link}: {e}\")\n",
    "            new_excluded.add(link)\n",
    "\n",
    "        # D√π gi·ªØ hay lo·∫°i, v·∫´n ƒë√°nh d·∫•u l√† ƒë√£ crawl\n",
    "        new_crawled.add(link)\n",
    "\n",
    "    # C·∫≠p nh·∫≠t d·ªØ li·ªáu\n",
    "    excluded_links.update(new_excluded)\n",
    "    crawled_links.update(new_crawled)\n",
    "    save_excluded_links(excluded_links)\n",
    "    save_crawled_links(crawled_links)\n",
    "\n",
    "    print(f\"üíæ ƒê√£ l∆∞u {len(new_crawled)} link m·ªõi v√†o danh s√°ch ƒë√£ crawl.\")\n",
    "    print(f\"‚úÖ T·ªïng s·ªë link h·ª£p l·ªá m·ªõi: {len(valid_links)}\")\n",
    "\n",
    "    return valid_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25acfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_singer_award(soup):\n",
    "    # S·ª≠ d·ª•ng set() ƒë·ªÉ t·ª± ƒë·ªông ch·ªëng tr√πng l·∫∑p l√† ch√≠nh x√°c!\n",
    "    awards = set()\n",
    "    tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "\n",
    "    for table in tables:\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all([\"td\", \"th\"])\n",
    "            if len(cells) < 2:\n",
    "                continue  # B·ªè qua h√†ng kh√¥ng c√≥ ƒë·ªß √¥\n",
    "\n",
    "            award_cell = cells[1]\n",
    "\n",
    "            # Logic g·ªëc c·ªßa b·∫°n: ch·ªâ l·∫•y nh·ªØng √¥ c√≥ th·∫ª <sup>\n",
    "            if award_cell.find(\"sup\"):\n",
    "                text_parts = []\n",
    "                # L·∫∑p qua c√°c n·ªôi dung con (g·ªìm text v√† tag)\n",
    "                for content in award_cell.contents:\n",
    "                    if content.name == \"sup\":\n",
    "                        break  # D·ª´ng l·∫°i khi g·∫∑p <sup> ƒë·∫ßu ti√™n\n",
    "                    \n",
    "                    # L·∫•y text, d√π n√≥ l√† chu·ªói tr·∫ßn hay n·∫±m trong tag kh√°c (nh∆∞ <a>, <b>)\n",
    "                    if isinstance(content, str):\n",
    "                        text_parts.append(content)\n",
    "                    elif hasattr(content, \"get_text\"):\n",
    "                        text_parts.append(content.get_text())\n",
    "\n",
    "                # 1. Gh√©p t·∫•t c·∫£ c√°c ph·∫ßn text l·∫°i\n",
    "                text = \"\".join(text_parts)\n",
    "                \n",
    "                # 2. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng:\n",
    "                #    Bi·∫øn \"Gi·∫£i th∆∞·ªüng\\n   Ca sƒ©\" th√†nh \"Gi·∫£i th∆∞·ªüng Ca sƒ©\"\n",
    "                text = \" \".join(text.split())\n",
    "\n",
    "                if text:\n",
    "                    awards.add(text)  # set s·∫Ω t·ª± lo vi·ªác ch·ªëng tr√πng l·∫∑p\n",
    "\n",
    "    return list(awards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crawled_links():\n",
    "    \"\"\"ƒê·ªçc danh s√°ch link ƒë√£ t·ª´ng crawl.\"\"\"\n",
    "    if os.path.exists(\"crawled_links.json\"):\n",
    "        with open(\"crawled_links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            return set(json.load(f))\n",
    "    return set()\n",
    "\n",
    "def save_crawled_links(data):\n",
    "    \"\"\"L∆∞u danh s√°ch link ƒë√£ crawl.\"\"\"\n",
    "    with open(\"crawled_links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(data), f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_valid_links(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    stop_headings = [\"Ch√∫ th√≠ch\", \"Tham kh·∫£o\", \"Li√™n k·∫øt ngo√†i\"]\n",
    "    content = soup.find('div', id='mw-content-text')\n",
    "    if not content:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung ch√≠nh trong trang!\")\n",
    "        return []\n",
    "\n",
    "    all_links = []\n",
    "    for element in content.find_all(['p', 'ul', 'ol', 'div', 'h2', 'h3'], recursive=True):\n",
    "        if element.name in ['h2', 'h3']:\n",
    "            heading_text = element.get_text(strip=True)\n",
    "            if any(stop in heading_text for stop in stop_headings):\n",
    "                print(f\"üõë D·ª´ng t·∫°i m·ª•c: {heading_text}\")\n",
    "                break\n",
    "        for link in element.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and not any(x in href for x in [':', '#']):\n",
    "                all_links.append(\"https://vi.wikipedia.org\" + href)\n",
    "\n",
    "    all_links = list(dict.fromkeys(all_links))\n",
    "    print(f\"üîç T√¨m th·∫•y {len(all_links)} ƒë∆∞·ªùng d·∫´n h·ª£p l·ªá trong trang ch√≠nh.\")\n",
    "\n",
    "    excluded_links = load_excluded_links()\n",
    "    crawled_links = load_crawled_links()\n",
    "\n",
    "    print(f\"üìÇ B·ªè qua {len(excluded_links)} link ƒë√£ b·ªã lo·∫°i tr∆∞·ªõc ƒë√≥...\")\n",
    "    print(f\"üß≠ B·ªè qua {len(crawled_links)} link ƒë√£ ƒë∆∞·ª£c crawl tr∆∞·ªõc ƒë√≥...\")\n",
    "\n",
    "    valid_links_new = []\n",
    "    new_excluded = set()\n",
    "    new_crawled = set()\n",
    "\n",
    "    keywords = [\n",
    "        \"ca sƒ© vi·ªát nam\", \"nam ca sƒ© vi·ªát nam\", \"n·ªØ ca sƒ© vi·ªát nam\",\n",
    "        \"ca sƒ© g·ªëc vi·ªát\", \"ca sƒ© h·∫£i ngo·∫°i\", \"nh·∫°c sƒ© vi·ªát nam\",\n",
    "        \"ban nh·∫°c vi·ªát nam\", \"ban nh·∫°c rock vi·ªát nam\",\n",
    "        \"nh√† s·∫£n xu·∫•t thu √¢m vi·ªát nam\", \"nh√† s·∫£n xu·∫•t √¢m nh·∫°c vi·ªát nam\",\n",
    "        \"nh·∫°c sƒ© h√≤a √¢m ph·ªëi kh√≠ vi·ªát nam\", \"rapper vi·ªát nam\"\n",
    "    ]\n",
    "    keywords_lower = [k.lower() for k in keywords]\n",
    "\n",
    "    for link in all_links:\n",
    "        if link in excluded_links or link in crawled_links:\n",
    "            print(f\"‚è© B·ªè qua (ƒë√£ lo·∫°i ho·∫∑c ƒë√£ crawl): {link}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sub_resp = requests.get(link, headers=headers, timeout=6)\n",
    "            sub_soup = BeautifulSoup(sub_resp.text, 'html.parser')\n",
    "            cat_div = sub_soup.find('div', id='mw-normal-catlinks')\n",
    "\n",
    "            if cat_div:\n",
    "                cat_text = cat_div.get_text(strip=True).lower()\n",
    "                if any(k in cat_text for k in keywords_lower):\n",
    "                    valid_links_new.append(link)\n",
    "                    print(f\"‚úÖ Gi·ªØ l·∫°i: {link}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Lo·∫°i b·ªè: {link}\")\n",
    "                    new_excluded.add(link)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y danh m·ª•c: {link}\")\n",
    "                new_excluded.add(link)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói khi truy c·∫≠p {link}: {e}\")\n",
    "            new_excluded.add(link)\n",
    "\n",
    "        new_crawled.add(link)\n",
    "\n",
    "    excluded_links.update(new_excluded)\n",
    "    crawled_links.update(new_crawled)\n",
    "    save_excluded_links(excluded_links)\n",
    "    save_crawled_links(crawled_links)\n",
    "\n",
    "    print(f\"üíæ ƒê√£ l∆∞u {len(new_crawled)} link m·ªõi v√†o danh s√°ch ƒë√£ crawl.\")\n",
    "    print(f\"‚úÖ T·ªïng s·ªë link h·ª£p l·ªá m·ªõi: {len(valid_links_new)}\")\n",
    "\n",
    "    # --- üîÅ B·ªï sung: Bao g·ªìm c·∫£ link h·ª£p l·ªá c≈© ---\n",
    "    valid_links_all = set(valid_links_new)\n",
    "\n",
    "    # ƒê·ªçc l·∫°i t·∫•t c·∫£ link ƒë√£ crawl, l·ªçc ra nh·ªØng link h·ª£p l·ªá c≈© (n·∫øu b·∫°n c√≥ danh s√°ch ri√™ng l∆∞u h·ª£p l·ªá)\n",
    "    # N·∫øu ch∆∞a c√≥, ta c√≥ th·ªÉ coi valid_links_all l√† t·ªïng h·ª£p hi·ªán t·∫°i\n",
    "    # ‚áí T·ª©c l√† ch·ªâ th√™m nh·ªØng link h·ª£p l·ªá c≈© (ƒë√£ ƒë∆∞·ª£c l∆∞u trong file kh√°c)\n",
    "    if os.path.exists(\"valid_links.json\"):\n",
    "        with open(\"valid_links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            old_valid = set(json.load(f))\n",
    "            valid_links_all.update(old_valid)\n",
    "\n",
    "    # C·∫≠p nh·∫≠t file l∆∞u link h·ª£p l·ªá t·ªïng h·ª£p\n",
    "    with open(\"valid_links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(valid_links_all), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"üì¶ T·ªïng h·ª£p t·∫•t c·∫£ link h·ª£p l·ªá: {len(valid_links_all)}\")\n",
    "\n",
    "    return list(valid_links_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_singer_info(start_urls, depth_limit=DEPTH_LIMIT):\n",
    "    singers = []\n",
    "    visited = set()  # tr√°nh tr√πng l·∫∑p\n",
    "    queue = deque([(url, depth_limit) for url in start_urls])\n",
    "\n",
    "    while queue:\n",
    "        url, depth = queue.popleft()  # l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu (BFS)\n",
    "        if url in visited or depth <= 0:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        try:\n",
    "            print(f\"Crawling {url} (depth={depth})...\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Tr·ªè v√†o info box\n",
    "            info_box = soup.find(\"table\", {\"class\": \"infobox\"})\n",
    "            if not info_box:\n",
    "                continue\n",
    "\n",
    "            info_rows = info_box.find_all(\"tr\")\n",
    "            singer_info = {}\n",
    "            singer_info['depth'] = depth\n",
    "            singer_info['name'] = soup.find(\"h1\", {\"id\": \"firstHeading\"}).get_text(strip=True)\n",
    "\n",
    "            for row in info_rows:\n",
    "                header = row.find(\"th\")\n",
    "                data = row.find(\"td\")\n",
    "                if header and data:\n",
    "                    key = header.get_text(strip=True)\n",
    "\n",
    "                    # --- TR∆Ø·ªúNG H·ª¢P 1: C√≥ <div class=\"hlist\"> ---\n",
    "                    hlist_div = data.find(\"div\", {\"class\": \"hlist\"})\n",
    "                    if hlist_div:\n",
    "                        items = [li.get_text(strip=True) for li in hlist_div.find_all(\"li\")]\n",
    "                        singer_info[key] = items\n",
    "                        continue\n",
    "\n",
    "                    # --- TR∆Ø·ªúNG H·ª¢P 2: C√≥ <ul> ---\n",
    "                    ul_tag = data.find(\"ul\")\n",
    "                    if ul_tag:\n",
    "                        items = [li.get_text(strip=True) for li in ul_tag.find_all(\"li\")]\n",
    "                        singer_info[key] = items\n",
    "                        continue\n",
    "\n",
    "                    # --- TR∆Ø·ªúNG H·ª¢P 3: C√≥ <br> ---\n",
    "                    if data.find(\"br\"):\n",
    "                        parts = [text.strip() for text in data.stripped_strings]\n",
    "                        singer_info[key] = [p for p in parts if p]\n",
    "                    else:\n",
    "                        value = data.get_text(separator=' ', strip=True)\n",
    "                        singer_info[key] = value\n",
    "\n",
    "            # --- Th√™m c√°c tr∆∞·ªùng b·ªï sung ---\n",
    "            singer_info['nƒÉm th√†nh l·∫≠p'], singer_info['nƒÉm tan r√£'] = get_years(singer_info.get('NƒÉm ho·∫°t ƒë·ªông'))\n",
    "            singer_info['link'] = url\n",
    "            singer_info['relations'] = []\n",
    "            singer_info['awards'] = crawl_singer_award(soup)\n",
    "\n",
    "            cat_div = soup.find('div', id='mw-normal-catlinks')\n",
    "\n",
    "            if cat_div:\n",
    "                cat_text = cat_div.get_text(strip=True).lower()\n",
    "                # if any cat_text have \"nam\", add singer_info['gi·ªõi t√≠nh'] = 'nam', \"n·ªØ\" t∆∞∆°ng t·ª±\n",
    "                if \"nam\" in cat_text:\n",
    "                    singer_info['gi·ªõi t√≠nh'] = 'nam'\n",
    "                elif \"n·ªØ\" in cat_text:\n",
    "                    singer_info['gi·ªõi t√≠nh'] = 'n·ªØ'\n",
    "\n",
    "            singers.append(singer_info)\n",
    "\n",
    "            # --- Th√™m c√°c ca sƒ© li√™n quan v√†o h√†ng ƒë·ª£i (n·∫øu c√≤n depth) ---\n",
    "            if depth - 1 > 0:\n",
    "                colab_links = crawl_valid_links(url)\n",
    "                singer_info['collaborated_singers'] = colab_links\n",
    "                for link in colab_links:\n",
    "                    if link not in visited:\n",
    "                        queue.append((link, depth - 1))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "    return singers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "singers = crawl_singer_info(SEED)\n",
    "print(singers)\n",
    "#save singer to JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57528b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename fields for better clarity\n",
    "for singer in singers:\n",
    "    if \"nƒÉm th√†nh l·∫≠p\" in singer and \"nƒÉm tan r√£\" in singer:\n",
    "        singer[\"nƒÉm ra m·∫Øt\"] = singer.pop(\"nƒÉm th√†nh l·∫≠p\", None)\n",
    "        singer[\"nƒÉm gi·∫£i ngh·ªá\"] = singer.pop(\"nƒÉm tan r√£\", None)\n",
    "\n",
    "    singer[\"Ngh·ªÅ nghi·ªáp kh√°c\"] = singer.pop(\"Ngh·ªÅ nghi·ªáp\", None)\n",
    "\n",
    "    # extract origin (if not present) from other related fields\n",
    "    if \"Nguy√™n qu√°n\" not in singer:\n",
    "        if \"Qu√™ qu√°n\" in singer:\n",
    "            singer[\"Nguy√™n qu√°n\"] = singer.pop(\"Qu√™ qu√°n\")\n",
    "        elif \"qu√™ qu√°n\" in singer:\n",
    "            singer[\"Nguy√™n qu√°n\"] = singer.pop(\"qu√™ qu√°n\")\n",
    "        elif \"N∆°i sinh\" in singer:\n",
    "            singer[\"Nguy√™n qu√°n\"] = singer.pop(\"N∆°i sinh\")\n",
    "        elif \"n∆°i sinh\" in singer:\n",
    "            singer[\"Nguy√™n qu√°n\"] = singer.pop(\"n∆°i sinh\")\n",
    "        else:\n",
    "            singer[\"Nguy√™n qu√°n\"] = \"\"\n",
    "\n",
    "    # --- Chu·∫©n h√≥a: ch·ªâ gi·ªØ ph·∫ßn tr∆∞·ªõc d·∫•u ph·∫©y (lo·∫°i b·ªè qu·ªëc gia) ---\n",
    "    origin = singer.get(\"Nguy√™n qu√°n\", \"\")\n",
    "    if isinstance(origin, str) and \",\" in origin:\n",
    "        # L·∫•y ph·∫ßn tr∆∞·ªõc d·∫•u ph·∫©y ƒë·∫ßu ti√™n\n",
    "        singer[\"Nguy√™n qu√°n\"] = origin.split(\",\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81612945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime \n",
    "\n",
    "date_pattern = re.compile(\n",
    "    r'(?P<day>\\d{1,2})[\\s\\xa0]*th√°ng[\\s\\xa0]*(?P<month>\\d{1,2})[,Ôºå\\s\\xa0]*(?:nƒÉm[\\s\\xa0]*)?(?P<year>\\d{4})',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "for singer in singers:\n",
    "    if \"ng√†y sinh\" in singer:\n",
    "        s = singer[\"ng√†y sinh\"]\n",
    "        if isinstance(s, str):\n",
    "            s = s.replace('\\xa0', ' ')\n",
    "            match = date_pattern.search(s)\n",
    "            if match:\n",
    "                print(f\"T√¨m th·∫•y: {match.group(0)}\")\n",
    "                day = int(match.group(\"day\"))\n",
    "                month = int(match.group(\"month\"))\n",
    "                year = int(match.group(\"year\"))\n",
    "                try:\n",
    "                    singer[\"ng√†y sinh\"] = datetime(year, month, day).date().isoformat()\n",
    "                except ValueError:\n",
    "                    singer[\"ng√†y sinh\"] = \"\"\n",
    "                continue\n",
    "\n",
    "    elif \"Sinh\" in singer:\n",
    "        sinh_data = singer.get(\"Sinh\", [])\n",
    "        if isinstance(sinh_data, list):\n",
    "            for s in sinh_data:\n",
    "                if not isinstance(s, str):\n",
    "                    continue\n",
    "                s = s.replace('\\xa0', ' ')  # üîß lo·∫°i b·ªè non-breaking space\n",
    "                print(f\"Ki·ªÉm tra chu·ªói: {s}\")\n",
    "                match = date_pattern.search(s)\n",
    "                if match:\n",
    "                    print(f\"T√¨m th·∫•y: {match.group(0)}\")\n",
    "                    day = int(match.group(\"day\"))\n",
    "                    month = int(match.group(\"month\"))\n",
    "                    year = int(match.group(\"year\"))\n",
    "                    try:\n",
    "                        singer[\"ng√†y sinh\"] = datetime(year, month, day).date().isoformat()\n",
    "                    except ValueError:\n",
    "                        singer[\"ng√†y sinh\"] = \"\"\n",
    "                    break\n",
    "    else:\n",
    "        singer[\"ng√†y sinh\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# M·∫´u regex x√≥a m·ªçi n·ªôi dung trong ngo·∫∑c (), [] c√πng kho·∫£ng tr·∫Øng k√®m theo\n",
    "clean_pattern = re.compile(r'\\s*(\\([^)]*\\)|\\[[^\\]]*\\])\\s*')\n",
    "\n",
    "for singer in singers:\n",
    "    name = singer.get(\"name\", \"\")\n",
    "    if not isinstance(name, str):\n",
    "        continue\n",
    "\n",
    "    # X√≥a n·ªôi dung trong ngo·∫∑c tr√≤n ho·∫∑c ngo·∫∑c vu√¥ng\n",
    "    cleaned = re.sub(clean_pattern, '', name).strip()\n",
    "\n",
    "    # X√≥a kho·∫£ng tr·∫Øng d∆∞ (n·∫øu c√≥ nhi·ªÅu kho·∫£ng tr·∫Øng)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "\n",
    "    singer[\"name\"] = cleaned\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ae7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex x√≥a n·ªôi dung trong ngo·∫∑c () ho·∫∑c [] c√πng kho·∫£ng tr·∫Øng k√®m theo\n",
    "clean_pattern = re.compile(r'\\s*(\\([^)]*\\)|\\[[^\\]]*\\])\\s*')\n",
    "\n",
    "for singer in singers:\n",
    "    data = singer.get(\"H√£ng ƒëƒ©a\", None)\n",
    "    if data is None:\n",
    "        singer[\"H√£ng ƒëƒ©a\"] = \"ƒê·ªôc l·∫≠p\"\n",
    "        continue\n",
    "\n",
    "    # N·∫øu l√† list\n",
    "    if isinstance(data, list):\n",
    "        cleaned_list = []\n",
    "        for item in data:\n",
    "            if not isinstance(item, str):\n",
    "                continue\n",
    "\n",
    "            # X√≥a n·ªôi dung trong ngo·∫∑c\n",
    "            cleaned = re.sub(clean_pattern, '', item).strip()\n",
    "            cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "\n",
    "            # B·ªè qua n·∫øu chu·ªói r·ªóng ho·∫∑c ch·ªâ ch·ª©a ngo·∫∑c, s·ªë, d·∫•u g·∫°ch\n",
    "            if not cleaned or re.fullmatch(r'[\\(\\)\\[\\]\\s\\d\\-‚Äì,]*', item.strip()):\n",
    "                continue\n",
    "\n",
    "            cleaned_list.append(cleaned)\n",
    "\n",
    "        singer[\"H√£ng ƒëƒ©a\"] = cleaned_list\n",
    "\n",
    "    # N·∫øu l√† chu·ªói\n",
    "    elif isinstance(data, str):\n",
    "        cleaned = re.sub(clean_pattern, '', data).strip()\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "\n",
    "        # N·∫øu ch·ªâ c√≥ ngo·∫∑c ho·∫∑c r·ªóng ‚Üí g√°n r·ªóng\n",
    "        if not cleaned or re.fullmatch(r'[\\(\\)\\[\\]\\s\\d\\-‚Äì,]*', data.strip()):\n",
    "            singer[\"H√£ng ƒëƒ©a\"] = \"\"\n",
    "        else:\n",
    "            singer[\"H√£ng ƒëƒ©a\"] = cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f94b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doi ten \"dong nhac\" thanh \"the loai\"\n",
    "for singer in singers:\n",
    "    if 'dong nhac' in singer:\n",
    "        singer['the loai'] = singer.pop('dong nhac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_relationships(singers):\n",
    "    # ƒê·∫£m b·∫£o c√≥ m·∫£ng 'relations'\n",
    "    for s in singers:\n",
    "        if 'relations' not in s:\n",
    "            s['relations'] = []\n",
    "\n",
    "    for singer in singers:\n",
    "        for link in singer.get('collaborated_singers', []):\n",
    "            colab_singer = next((s for s in singers if s['link'] == link), None)\n",
    "            if not colab_singer:\n",
    "                continue\n",
    "\n",
    "            relation_types = []\n",
    "\n",
    "            # --- C√πng th·ªÉ lo·∫°i ---\n",
    "            singer_genres = set(singer.get(\"Th·ªÉ lo·∫°i\", []))\n",
    "            colab_genres = set(colab_singer.get(\"Th·ªÉ lo·∫°i\", []))\n",
    "            if singer_genres and colab_genres and singer_genres.intersection(colab_genres):\n",
    "                relation_types.append(\"same_genre\")\n",
    "\n",
    "            # --- C·ªông t√°c ---\n",
    "            relation_types.append(\"collaborated\")\n",
    "\n",
    "            # --- C√πng nguy√™n qu√°n ---\n",
    "            if singer.get(\"Nguy√™n qu√°n\") and colab_singer.get(\"Nguy√™n qu√°n\"):\n",
    "                if singer[\"Nguy√™n qu√°n\"] == colab_singer[\"Nguy√™n qu√°n\"]:\n",
    "                    relation_types.append(\"same_origin\")\n",
    "\n",
    "            # --- C√πng h√£ng ƒëƒ©a ---\n",
    "            label_a = singer.get(\"H√£ng ƒëƒ©a\")\n",
    "            label_b = colab_singer.get(\"H√£ng ƒëƒ©a\")\n",
    "\n",
    "            def normalize_label(label):\n",
    "                \"\"\"Chuy·ªÉn v·ªÅ chu·ªói th∆∞·ªùng, lo·∫°i b·ªè kho·∫£ng tr·∫Øng v√† l·∫•y ph·∫ßn ƒë·∫ßu n·∫øu l√† danh s√°ch\"\"\"\n",
    "                if not label:\n",
    "                    return \"\"\n",
    "                if isinstance(label, list):\n",
    "                    label = label[0] if label else \"\"\n",
    "                return str(label).strip().lower()\n",
    "\n",
    "            label_a_norm = normalize_label(label_a)\n",
    "            label_b_norm = normalize_label(label_b)\n",
    "\n",
    "            if label_a_norm and label_b_norm and label_a_norm == label_b_norm and label_a_norm != \"ƒë·ªôc l·∫≠p\":\n",
    "                relation_types.append(\"same_label\")\n",
    "\n",
    "            # --- C√πng n∆°i ƒë√†o t·∫°o ---\n",
    "            if singer.get(\"ƒê√†o t·∫°o\") and colab_singer.get(\"ƒê√†o t·∫°o\"):\n",
    "                if singer[\"ƒê√†o t·∫°o\"] == colab_singer[\"ƒê√†o t·∫°o\"]:\n",
    "                    relation_types.append(\"same_institution\")\n",
    "\n",
    "            # --- Th√™m quan h·ªá v√†o singer ---\n",
    "            existing_relation = next((r for r in singer['relations'] if r['singer_link'] == link), None)\n",
    "            if existing_relation:\n",
    "                for t in relation_types:\n",
    "                    if t not in existing_relation['type']:\n",
    "                        existing_relation['type'].append(t)\n",
    "            else:\n",
    "                singer['relations'].append({\n",
    "                    'singer_link': link,\n",
    "                    'type': relation_types\n",
    "                })\n",
    "\n",
    "            # --- Quan h·ªá ng∆∞·ª£c ---\n",
    "            reverse_relation = next((r for r in colab_singer['relations'] if r['singer_link'] == singer['link']), None)\n",
    "            if reverse_relation:\n",
    "                for t in relation_types:\n",
    "                    if t not in reverse_relation['type']:\n",
    "                        reverse_relation['type'].append(t)\n",
    "            else:\n",
    "                colab_singer['relations'].append({\n",
    "                    'singer_link': singer['link'],\n",
    "                    'type': relation_types.copy()\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_relationships(singers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101018c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../raw_data/singers.json', 'r', encoding='utf-8') as f:\n",
    "    singers = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18138e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for singer in singers:\n",
    "    singer.pop(\"Sinh\", None)\n",
    "    singer.pop(\"Ph·ªëi ng·∫´u\", None)\n",
    "    singer.pop(\"Nh·∫°c c·ª•\", None)\n",
    "    singer.pop(\"B·∫°n ƒë·ªùi\", None)\n",
    "    singer.pop(\"Con c√°i\", None)\n",
    "    singer.pop(\"Gi·∫£i th∆∞·ªüng\", None)\n",
    "    singer.pop(\"Website\", None)\n",
    "    singer.pop(\"collaborated_singers\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1624de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../raw_data/singers.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(singers, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
