{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from collections import deque\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcce334",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = r\"../raw_data\"\n",
    "EXCLUDED_PATH = os.path.join(SAVE_DIR, \"excluded_links.json\")\n",
    "DEPTH_LIMIT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75198742",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5744987",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [\"https://vi.wikipedia.org/wiki/L%E1%BB%87_Quy%C3%AAn_(ca_s%C4%A9,_sinh_1981)\", \n",
    "        \"https://vi.wikipedia.org/wiki/Miu_L%C3%AA\", \n",
    "        \"https://vi.wikipedia.org/wiki/H%C3%B2a_Minzy\",\n",
    "        \"https://vi.wikipedia.org/wiki/M%E1%BB%B9_Linh\",\n",
    "        \"https://vi.wikipedia.org/wiki/Only_C\",\n",
    "        \"https://vi.wikipedia.org/wiki/JustaTee\",\n",
    "        \"https://vi.wikipedia.org/wiki/Ch%E1%BA%BF_Linh\",\n",
    "        \"https://vi.wikipedia.org/wiki/%C4%90%C3%A0m_V%C4%A9nh_H%C6%B0ng\",\n",
    "        \"https://vi.wikipedia.org/wiki/Mr._Siro\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters(text):\n",
    "    if isinstance(text, str):\n",
    "        # B·ªè c·ª•m [s·ª≠a|s·ª≠a m√£ ngu·ªìn]\n",
    "        cleaned = text.replace(\"[s·ª≠a|s·ª≠a m√£ ngu·ªìn]\", \"\")\n",
    "        # Xo√° ngo·∫∑c v√† d·∫•u c√°ch/d·∫•u ngo·∫∑c k√©p ·ªü ƒë·∫ßu & cu·ªëi\n",
    "        cleaned = re.sub(r'^[\\s\\(\\)\\[\\]\\'\"]+|[\\s\\(\\)\\[\\]\\'\"]+$', '', cleaned)\n",
    "        return cleaned.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years(active_years):\n",
    "    is_active = False\n",
    "    if not active_years:\n",
    "        return None, None\n",
    "\n",
    "    if isinstance(active_years, str):\n",
    "        active_years = [active_years]\n",
    "\n",
    "    start_years = []\n",
    "    end_years = []\n",
    "\n",
    "    for period in active_years:\n",
    "        if not period:\n",
    "            continue\n",
    "        p = period.strip()\n",
    "\n",
    "        # Chu·∫©n ho√° c√°c lo·∫°i dash th√†nh hyphen th∆∞·ªùng\n",
    "        p = re.sub(r'[‚Äì‚Äî‚àí]', '-', p)\n",
    "\n",
    "        # üîπ L·∫•y t·∫•t c·∫£ nƒÉm v√† c·∫£ t·ª´ \"nay\"\n",
    "        tokens = re.findall(r'\\b(?:19|20)\\d{2}\\b|\\b(?:nay|hi·ªán t·∫°i|present|now)\\b', p, re.IGNORECASE)\n",
    "\n",
    "        # N·∫øu kh√¥ng c√≥ token n√†o, th·ª≠ ki·ªÉm tra d·∫°ng ƒë·∫∑c bi·ªát \"2015-\"\n",
    "        if not tokens:\n",
    "            if re.search(r'\\b(?:19|20)\\d{2}\\b\\s*-\\s*$', p):\n",
    "                start = int(re.search(r'(?:19|20)\\d{2}', p).group())\n",
    "                start_years.append(start)\n",
    "            continue\n",
    "\n",
    "        # X·ª≠ l√Ω token ƒë·∫ßu ti√™n (nƒÉm b·∫Øt ƒë·∫ßu)\n",
    "        first = tokens[0]\n",
    "        if re.match(r'(?:19|20)\\d{2}', first):\n",
    "            start_years.append(int(first))\n",
    "\n",
    "        # X·ª≠ l√Ω token cu·ªëi c√πng (nƒÉm tan r√£)\n",
    "        last = tokens[-1]\n",
    "        if re.match(r'(?:19|20)\\d{2}', last):\n",
    "            end_years.append(int(last))\n",
    "        elif re.match(r'(nay|hi·ªán t·∫°i|present|now)', last, re.IGNORECASE):\n",
    "            is_active = True\n",
    "            # n·∫øu l√† 'nay' th√¨ kh√¥ng c√≥ nƒÉm tan r√£\n",
    "            pass\n",
    "        elif len(tokens) == 1:\n",
    "            # ch·ªâ c√≥ m·ªôt nƒÉm, coi l√† ho·∫°t ƒë·ªông trong nƒÉm ƒë√≥\n",
    "            end_years.append(int(first))\n",
    "\n",
    "    if not start_years:\n",
    "        return None, None\n",
    "\n",
    "    start = min(start_years)\n",
    "    end = None if is_active else max(end_years)\n",
    "    return start, end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excluded_links():\n",
    "    if os.path.exists(EXCLUDED_PATH):\n",
    "        with open(EXCLUDED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            return set(json.load(f))\n",
    "    return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excluded_links(excluded_links):\n",
    "    with open(EXCLUDED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted(list(excluded_links)), f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_valid_links(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ‚ùå C√°c ph·∫ßn c·∫ßn b·ªè qua\n",
    "    stop_headings = [\"Ch√∫ th√≠ch\", \"Tham kh·∫£o\", \"Li√™n k·∫øt ngo√†i\"]\n",
    "\n",
    "    content = soup.find('div', id='mw-content-text')\n",
    "    if not content:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung ch√≠nh trong trang!\")\n",
    "        return []\n",
    "\n",
    "    all_links = []\n",
    "\n",
    "    for element in content.find_all(['p', 'ul', 'ol', 'div', 'h2', 'h3'], recursive=True):\n",
    "        # N·∫øu g·∫∑p heading d·ª´ng, th√¨ d·ª´ng lu√¥n vi·ªác duy·ªát\n",
    "        if element.name in ['h2', 'h3']:\n",
    "            heading_text = element.get_text(strip=True)\n",
    "            if any(stop in heading_text for stop in stop_headings):\n",
    "                print(f\"üõë D·ª´ng t·∫°i m·ª•c: {heading_text}\")\n",
    "                break\n",
    "\n",
    "        # L·∫•y link trong c√°c ƒëo·∫°n c√≤n l·∫°i\n",
    "        for link in element.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and not any(x in href for x in [':', '#']):\n",
    "                full_url = \"https://vi.wikipedia.org\" + href\n",
    "                all_links.append(full_url)\n",
    "\n",
    "    # Lo·∫°i b·ªè tr√πng l·∫∑p\n",
    "    all_links = list(dict.fromkeys(all_links))\n",
    "    print(f\"üîç T√¨m th·∫•y {len(all_links)} ƒë∆∞·ªùng d·∫´n h·ª£p l·ªá trong trang ch√≠nh.\")\n",
    "\n",
    "    excluded_links = load_excluded_links()\n",
    "    print(f\"üìÇ B·ªè qua {len(excluded_links)} link ƒë√£ b·ªã lo·∫°i tr∆∞·ªõc ƒë√≥...\")\n",
    "\n",
    "    valid_links = []\n",
    "    new_excluded = set()\n",
    "    keywords = [\n",
    "        \"ca sƒ© vi·ªát nam\", \"nam ca sƒ© vi·ªát nam\", \"n·ªØ ca sƒ© vi·ªát nam\",\n",
    "        \"ca sƒ© g·ªëc vi·ªát\", \"ca sƒ© h·∫£i ngo·∫°i\", \"nh·∫°c sƒ© vi·ªát nam\"\n",
    "    ]\n",
    "\n",
    "    # ƒê∆∞a t·∫•t c·∫£ keywords v·ªÅ d·∫°ng ch·ªØ th∆∞·ªùng\n",
    "    keywords_lower = [k.lower() for k in keywords]\n",
    "\n",
    "    for link in all_links:\n",
    "        if link in excluded_links:\n",
    "            print(f\"‚è© B·ªè qua (ƒë√£ lo·∫°i tr∆∞·ªõc): {link}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sub_resp = requests.get(link, headers=headers, timeout=6)\n",
    "            sub_soup = BeautifulSoup(sub_resp.text, 'html.parser')\n",
    "            cat_div = sub_soup.find('div', id='mw-normal-catlinks')\n",
    "\n",
    "            if cat_div:\n",
    "                cat_text = cat_div.get_text(strip=True).lower()\n",
    "                if any(k in cat_text for k in keywords_lower):\n",
    "                    valid_links.append(link)\n",
    "                    print(f\"‚úÖ Gi·ªØ l·∫°i: {link}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Lo·∫°i b·ªè: {link}\")\n",
    "                    new_excluded.add(link)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y danh m·ª•c: {link}\")\n",
    "                new_excluded.add(link)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói khi truy c·∫≠p {link}: {e}\")\n",
    "            new_excluded.add(link)\n",
    "\n",
    "    excluded_links.update(new_excluded)\n",
    "    save_excluded_links(excluded_links)\n",
    "\n",
    "    return valid_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_singer_info(start_urls, depth_limit=DEPTH_LIMIT):\n",
    "    singers = []\n",
    "    visited = set()  # tr√°nh tr√πng l·∫∑p\n",
    "    queue = deque([(url, depth_limit) for url in start_urls])\n",
    "\n",
    "    while queue:\n",
    "        url, depth = queue.popleft()  # l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu (BFS)\n",
    "        if url in visited or depth <= 0:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        try:\n",
    "            print(f\"Crawling {url} (depth={depth})...\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Tr·ªè v√†o info box\n",
    "            info_box = soup.find(\"table\", {\"class\": \"infobox\"})\n",
    "            if not info_box:\n",
    "                continue\n",
    "\n",
    "            info_rows = info_box.find_all(\"tr\")\n",
    "            singer_info = {}\n",
    "            singer_info['depth'] = depth\n",
    "            singer_info['name'] = soup.find(\"h1\", {\"id\": \"firstHeading\"}).get_text(strip=True)\n",
    "\n",
    "            for row in info_rows:\n",
    "                header = row.find(\"th\")\n",
    "                data = row.find(\"td\")\n",
    "                if header and data:\n",
    "                    key = header.get_text(strip=True)\n",
    "\n",
    "                    # --- TR∆Ø·ªúNG H·ª¢P 1: C√≥ <div class=\"hlist\"> ---\n",
    "                    hlist_div = data.find(\"div\", {\"class\": \"hlist\"})\n",
    "                    if hlist_div:\n",
    "                        items = [li.get_text(strip=True) for li in hlist_div.find_all(\"li\")]\n",
    "                        singer_info[key] = items\n",
    "                        continue\n",
    "\n",
    "                    # --- TR∆Ø·ªúNG H·ª¢P 2: C√≥ <ul> ---\n",
    "                    ul_tag = data.find(\"ul\")\n",
    "                    if ul_tag:\n",
    "                        items = [li.get_text(strip=True) for li in ul_tag.find_all(\"li\")]\n",
    "                        singer_info[key] = items\n",
    "                        continue\n",
    "\n",
    "                    # --- TR∆Ø·ªúNG H·ª¢P 3: C√≥ <br> ---\n",
    "                    if data.find(\"br\"):\n",
    "                        parts = [text.strip() for text in data.stripped_strings]\n",
    "                        singer_info[key] = [p for p in parts if p]\n",
    "                    else:\n",
    "                        value = data.get_text(separator=' ', strip=True)\n",
    "                        singer_info[key] = value\n",
    "\n",
    "            # --- Th√™m c√°c tr∆∞·ªùng b·ªï sung ---\n",
    "            singer_info['nƒÉm th√†nh l·∫≠p'], singer_info['nƒÉm tan r√£'] = get_years(singer_info.get('NƒÉm ho·∫°t ƒë·ªông'))\n",
    "            singer_info['link'] = url\n",
    "            singer_info['relations'] = []\n",
    "\n",
    "            singers.append(singer_info)\n",
    "\n",
    "            # --- Th√™m c√°c ca sƒ© li√™n quan v√†o h√†ng ƒë·ª£i (n·∫øu c√≤n depth) ---\n",
    "            if depth - 1 > 0:\n",
    "                colab_links = crawl_valid_links(url)\n",
    "                singer_info['collaborated_singers'] = colab_links\n",
    "                for link in colab_links:\n",
    "                    if link not in visited:\n",
    "                        queue.append((link, depth - 1))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "    return singers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "singers = crawl_singer_info(SEED)\n",
    "print(singers)\n",
    "#save singer to JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11949f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_relationships(singers):\n",
    "    for singer in singers:\n",
    "        for link in singer.get('collaborated_singers', []):\n",
    "            colab_singer = next((s for s in singers if s['link'] == link), None)\n",
    "            if colab_singer:\n",
    "                is_same_genre = False  \n",
    "                for genre in singer.get(\"Th·ªÉ lo·∫°i\", []):\n",
    "                    if \"Th·ªÉ lo·∫°i\" in colab_singer and genre in colab_singer[\"Th·ªÉ lo·∫°i\"]:\n",
    "                        is_same_genre = True\n",
    "                        break\n",
    "                if is_same_genre:\n",
    "                    singer['relations'].append({\n",
    "                        'singer_link': link,\n",
    "                        'type': 'same_genre'\n",
    "                    })\n",
    "                    colab_singer['relations'].append({\n",
    "                        'singer_link': singer['link'],\n",
    "                        'type': 'same_genre'\n",
    "                    })\n",
    "                else:\n",
    "                    singer['relations'].append({\n",
    "                        'singer_link': link,\n",
    "                        'type': 'collaborated'\n",
    "                    })\n",
    "                    colab_singer['relations'].append({\n",
    "                        'singer_link': singer['link'],\n",
    "                        'type': 'collaborated'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f786678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "identify_relationships(singers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../raw_data/singer_demo.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(singers, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96950c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(singers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce982b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword c·ªßa nh·ªØng ca sƒ© VNCH\n",
    "south_vietnamese_keyword = [\n",
    "    \"vi·ªát nam c·ªông ho√†\", \"s√†i g√≤n\", \"nam k√¨\", \"nam k·ª≥\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766e11b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'singers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# T√¨m c√°c ca sƒ© thu·ªôc VNCH\u001b[39;00m\n\u001b[32m      2\u001b[39m south_vietnamese_singers = [\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     singer \u001b[38;5;28;01mfor\u001b[39;00m singer \u001b[38;5;129;01min\u001b[39;00m \u001b[43msingers\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m      5\u001b[39m         key \u001b[38;5;129;01min\u001b[39;00m singer \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m      6\u001b[39m             (\u001b[38;5;28misinstance\u001b[39m(singer[key], \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m      7\u001b[39m                 \u001b[38;5;28many\u001b[39m(kw.lower() \u001b[38;5;129;01min\u001b[39;00m item.lower() \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m south_vietnamese_keyword)\n\u001b[32m      8\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m singer[key]\n\u001b[32m      9\u001b[39m             )) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m     10\u001b[39m             (\u001b[38;5;28misinstance\u001b[39m(singer[key], \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m     11\u001b[39m                 kw.lower() \u001b[38;5;129;01min\u001b[39;00m singer[key].lower() \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m south_vietnamese_keyword\n\u001b[32m     12\u001b[39m             ))\n\u001b[32m     13\u001b[39m         )\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mQu·ªëc t·ªãch\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mN∆°i sinh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mN∆°i ho·∫°t ƒë·ªông\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m ]\n",
      "\u001b[31mNameError\u001b[39m: name 'singers' is not defined"
     ]
    }
   ],
   "source": [
    "# T√¨m c√°c ca sƒ© thu·ªôc VNCH\n",
    "south_vietnamese_singers = [\n",
    "    singer for singer in singers\n",
    "    if any(\n",
    "        key in singer and (\n",
    "            (isinstance(singer[key], list) and any(\n",
    "                any(kw.lower() in item.lower() for kw in south_vietnamese_keyword)\n",
    "                for item in singer[key]\n",
    "            )) or\n",
    "            (isinstance(singer[key], str) and any(\n",
    "                kw.lower() in singer[key].lower() for kw in south_vietnamese_keyword\n",
    "            ))\n",
    "        )\n",
    "        for key in [\"Qu·ªëc t·ªãch\", \"N∆°i sinh\", \"N∆°i ho·∫°t ƒë·ªông\"]\n",
    "    )\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
