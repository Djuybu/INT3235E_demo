import requests
from bs4 import BeautifulSoup
import json
import os

SAVE_DIR = r"C:\Users\Woangchung\INT3235E_demo\raw_data"
EXCLUDED_PATH = os.path.join(SAVE_DIR, "excluded_links.json")

def load_excluded_links():
    if os.path.exists(EXCLUDED_PATH):
        with open(EXCLUDED_PATH, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_excluded_links(excluded_links):
    with open(EXCLUDED_PATH, "w", encoding="utf-8") as f:
        json.dump(sorted(list(excluded_links)), f, ensure_ascii=False, indent=2)

def crawl_valid_links(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # CÃ¡c má»¥c heading Ä‘Æ°á»£c phÃ©p láº¥y link phÃ­a sau
    target_headings = [
        "Cuá»™c Ä‘á»i sá»± nghiá»‡p",
        "Phong cÃ¡ch nghá»‡ thuáº­t",
        "HÃ¬nh tÆ°á»£ng cÃ´ng chÃºng",
        "Danh sÃ¡ch Ä‘Ä©a nháº¡c"
    ]

    valid_sections = []
    for heading in soup.find_all(['div'], class_=['mw-heading2', 'mw-heading3']):
        text = heading.get_text(strip=True)
        if text in target_headings:
            valid_sections.append(heading)

    all_links = []
    for section in valid_sections:
        for sibling in section.find_all_next():
            if sibling.name in ['div'] and 'mw-heading' in ' '.join(sibling.get('class', [])):
                break  # Dá»«ng khi gáº·p heading khÃ¡c
            links = sibling.find_all('a', href=True)
            for link in links:
                href = link['href']
                if href.startswith('/wiki/') and not any(x in href for x in [':', '#']):
                    full_url = "https://vi.wikipedia.org" + href
                    all_links.append(full_url)

    all_links = list(dict.fromkeys(all_links))
    print(f"ğŸ” TÃ¬m tháº¥y {len(all_links)} Ä‘Æ°á»ng dáº«n sau cÃ¡c má»¥c yÃªu cáº§u.")

    excluded_links = load_excluded_links()
    print(f"ğŸ“‚ Äang bá» qua {len(excluded_links)} link Ä‘Ã£ bá»‹ loáº¡i trÆ°á»›c Ä‘Ã³...")

    valid_links = []
    new_excluded = set()
    keywords = ["ca sÄ© Viá»‡t Nam", "nam ca sÄ© Viá»‡t Nam", "ná»¯ ca sÄ© Viá»‡t Nam"]

    for link in all_links:
        if link in excluded_links:
            print(f"â© Bá» qua (Ä‘Ã£ loáº¡i trÆ°á»›c): {link}")
            continue

        try:
            sub_resp = requests.get(link, headers=headers, timeout=5)
            sub_soup = BeautifulSoup(sub_resp.text, 'html.parser')
            cat_div = sub_soup.find('div', id='mw-normal-catlinks')

            if cat_div and any(k in cat_div.get_text() for k in keywords):
                valid_links.append(link)
                print(f"âœ… Giá»¯ láº¡i: {link}")
            else:
                print(f"âŒ Loáº¡i bá»: {link}")
                new_excluded.add(link)

        except Exception as e:
            print(f"âš ï¸ Lá»—i khi truy cáº­p {link}: {e}")
            new_excluded.add(link)

    excluded_links.update(new_excluded)
    save_excluded_links(excluded_links)

    return valid_links


if __name__ == "__main__":
    os.makedirs(SAVE_DIR, exist_ok=True)

    url = input("ğŸ”— Nháº­p URL Wikipedia: ").strip()
    name = input("ğŸ“ Äáº·t tÃªn file JSON (vÃ­ dá»¥: male, female, band...): ").strip()
    save_file = os.path.join(SAVE_DIR, f"singers_{name}.json")

    links = crawl_valid_links(url)

    with open(save_file, "w", encoding="utf-8") as f:
        json.dump(links, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ÄÃ£ lÆ°u {len(links)} Ä‘Æ°á»ng dáº«n há»£p lá»‡ vÃ o: {save_file}")
    print(f"ğŸ“ Danh sÃ¡ch link loáº¡i bá» Ä‘Æ°á»£c lÆ°u táº¡i: {EXCLUDED_PATH}")
